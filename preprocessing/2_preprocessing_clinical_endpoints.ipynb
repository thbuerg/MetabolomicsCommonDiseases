{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.436340Z",
     "start_time": "2020-11-04T12:31:48.732042Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = \"name_of_your_dataset\"\n",
    "path = \"/path/to/mapping/files\"\n",
    "data_path = \"/path/to/decoded/output\"\n",
    "dataset_path = f\"{data_path}/2_datasets_pre/{dataset_name}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.895222Z",
     "start_time": "2020-11-04T12:31:49.891332Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:33:14.171198Z",
     "start_time": "2020-11-04T12:31:50.204540Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{data_path}/1_decoded/ukb_data_210517.feather\")\n",
    "data_field = pd.read_feather(f\"{data_path}/1_decoded/ukb_data_field_210517.feather\")\n",
    "data_columns = data.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings + Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.867152Z",
     "start_time": "2020-11-04T12:33:16.878773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop obvious missing data\n",
    "print(len(data))\n",
    "data = data.dropna(subset=[\"sex_f31_0_0\"], axis=0)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.872216Z",
     "start_time": "2020-11-04T12:34:05.869505Z"
    }
   },
   "outputs": [],
   "source": [
    "time0_col=\"date_of_attending_assessment_centre_f53_0_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.889725Z",
     "start_time": "2020-11-04T12:34:05.874587Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fields(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields) & data_field[\"field.tab\"].str.contains(\"f\\\\.\\\\d+\\\\.0\\\\.\\\\d\")].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_fields_all(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields)].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_data_fields(fields, data, data_field):\n",
    "    f = get_fields(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()\n",
    "\n",
    "def get_data_fields_all(fields, data, data_field):\n",
    "    f = get_fields_all(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.667281Z",
     "start_time": "2020-11-04T12:36:14.427693Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_dir = f\"{data_path}/mapping/athena\"\n",
    "vocab = {\n",
    "    \"concept\": pd.read_csv(f\"{vocab_dir}/CONCEPT.csv\", sep='\\t'),\n",
    "    \"domain\": pd.read_csv(f\"{vocab_dir}/DOMAIN.csv\", sep='\\t'),\n",
    "    \"class\": pd.read_csv(f\"{vocab_dir}/CONCEPT_CLASS.csv\", sep='\\t'),\n",
    "    \"relationship\": pd.read_csv(f\"{vocab_dir}/RELATIONSHIP.csv\", sep='\\t'),\n",
    "    \"drug_strength\": pd.read_csv(f\"{vocab_dir}/DRUG_STRENGTH.csv\", sep='\\t'),\n",
    "    \"vocabulary\": pd.read_csv(f\"{vocab_dir}/VOCABULARY.csv\", sep='\\t'),\n",
    "    \"concept_synonym\": pd.read_csv(f\"{vocab_dir}/CONCEPT_SYNONYM.csv\", sep='\\t'),\n",
    "    \"concept_ancestor\": pd.read_csv(f\"{vocab_dir}/CONCEPT_ANCESTOR.csv\", sep='\\t'),\n",
    "    \"concept_relationship\": pd.read_csv(f\"{vocab_dir}/CONCEPT_RELATIONSHIP.csv\", sep='\\t')                       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.772869Z",
     "start_time": "2020-11-04T12:37:14.669541Z"
    }
   },
   "outputs": [],
   "source": [
    "coding1836 = pd.read_csv(f\"{path}/codings/coding1836.tsv\", sep=\"\\t\").rename(columns={\"coding\":\"code\"})\n",
    "phecodes = pd.read_csv(f\"{path}/phecodes/phecode_icd10.csv\")\n",
    "def phenotype_children(phecodes, phenotype_list):\n",
    "    l={}\n",
    "    phecodes = phecodes.dropna(subset=[\"Phenotype\"], axis=0)\n",
    "    for ph, ph_names in phenotype_list.items():\n",
    "        regex = \"|\".join(ph_names)\n",
    "        l[ph] = list(phecodes[phecodes.Phenotype.str.contains(regex, case=False)].ICD10.str.replace(\"\\\\.\", \"\").str.slice(0, 3).unique())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes = pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses_codes.feather')).drop(\"level\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_codes = pd.read_feather(f\"{data_path}/1_decoded/codes_death_records_210115.feather\").query(\"level==1\").drop(\"level\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_codes = pd.concat([diagnoses_codes, death_codes[diagnoses_codes.columns]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.628580Z",
     "start_time": "2020-11-04T12:33:33.036Z"
    }
   },
   "outputs": [],
   "source": [
    "### define in snomed and get icd codes from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hospital admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_list = {\n",
    "    \"myocardial_infarction\": ['I21', 'I22', 'I23', 'I24', 'I25'],\n",
    "    \"stroke\": ['G45', \"I63\", \"I64\"],\n",
    "    \"diabetes\" : ['E10', 'E11', 'E12', 'E13', 'E14'],\n",
    "    \"diabetes1\" : ['E10'],\n",
    "    \"diabetes2\" : ['E11', 'E12', 'E13', 'E14'],\n",
    "    \"atrial_fibrillation\": ['I47', 'I48'],\n",
    "    'migraine': ['G43', 'G44'],\n",
    "    'rheumatoid_arthritis': ['J99', 'M05', 'M06', 'M08', 'M12', 'M13'],\n",
    "    \"systemic_lupus_erythematosus\": ['M32'],\n",
    "    'severe_mental_illness': ['F20', 'F25', 'F30', 'F31', 'F32', 'F33', 'F44'],\n",
    "    \"erectile_dysfunction\" : ['F52', 'N48'],  \n",
    "    \"chronic_kidney_disease\": [\"I12\", \"N18\", \"N19\"],\n",
    "    \"liver_disease\":[\"K70\", \"K71\", \"K72\", \"K73\", \"K74\", \"K75\", \"K76\", \"K77\"],\n",
    "    \"dementia\":['F00', 'F01', 'F02', 'F03'],\n",
    "    \"copd\": ['J44'],\n",
    "    \"M_all_cause_dementia\": [\"F00\", \"F01\", \"F02\", \"F03\", \"G30\", \"G31\"],\n",
    "    \"M_MACE\": [\"G45\", \"I21\", \"I22\", \"I23\", \"I24\", \"I25\", \"I63\", \"I64\"],\n",
    "    \"M_type_2_diabetes\": [\"E10\", \"E11\", \"E12\", \"E13\", \"E14\"],\n",
    "    \"M_liver_disease\": [\"B15\", \"B16\", \"B17\", \"B18\", \"B19\", \"C22\", \"E83\", \"E88\", \"I85\", \n",
    "                          \"K70\", \"K72\", \"K73\", \"K74\", \"K75\", \"K76\", \"R18\", \"Z94\"],\n",
    "    \"M_renal_disease\":  [f\"N{i:02}\" for i in range(20)]+[f\"N{i:02}\" for i in range(25, 30)],\n",
    "    \"M_atrial_fibrillation\": [\"I48\"],\n",
    "    \"M_heart_failure\":[\"I50\"],\n",
    "    \"M_coronary_heart_disease\": [f\"I{i:02}\" for i in range(20, 26)],\n",
    "    \"M_venous_thrombosis\": [\"I80\", \"I81\", \"I82\"],\n",
    "    \"M_cerebral_stroke\":[\"I63\", \"I65\", \"I66\"],\n",
    "    \"M_haemorrhagic_stroke\": [\"I60, I61, I62\"],\n",
    "    \"M_abdominal_aortic_aneurysm\" : [\"I71\"],\n",
    "    \"M_peripheral_arterial_disease\": ['I70', 'I71', 'I72', 'I73', 'I74', 'I75', 'I76', 'I77', 'I78', 'I79'],\n",
    "    \"M_asthma\":[\"J45\", \"J46\"],\n",
    "    \"M_chronic_obstructuve_pulmonary_disease\":[\"J40\", \"J41\", \"J42\", \"J43\", \"J44\", \"J47\"],\n",
    "    \"M_lung_cancer\":[\"C33\", \"C34\"],\n",
    "    \"M_non_melanoma_skin_cancer\":[\"C44\"],\n",
    "    \"M_stomach_cancer\":[\"C16\"],\n",
    "    \"M_oesophagus_cancer\":[\"C15\"],\n",
    "    \"M_colon_cancer\":[\"C18\"],\n",
    "    \"M_rectal_cancer\":[\"C19\", \"C20\"],\n",
    "    \"M_prostate_cancer\":[\"C61\"],\n",
    "    \"M_ovarian_cancer\":[\"C56\", \"C57\"],\n",
    "    \"M_breast_cancer\":[\"C50\"],\n",
    "    \"M_uterus_cancer\":[\"C54\"],\n",
    "    \"M_parkinsons_disease\":[\"G20\", \"G21\", \"G22\"],\n",
    "    \"M_fractures\":[\"S02\", \"S12\", \"S22\", \"S32\", \"S42\", \"S52\", \"S62\", \"S72\", \"S82\", \"S92\", \"T02\", \"T08\", \"T10\"],\n",
    "    \"M_cataracts\":[\"H25\", \"H26\"],\n",
    "    \"M_glaucoma\":[\"H40\"]  \n",
    "}\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'endpoint_list.yaml'), 'w') as file:\n",
    "    yaml.dump(endpoint_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime\n",
    "\n",
    "def extract_endpoints_tte(data, diagnoses_codes, endpoint_list, time0_col, level=None):\n",
    "    if level is not None: diagnoses_codes = diagnoses_codes.query(\"level==@level\")\n",
    "    diagnoses_codes_time0 = diagnoses_codes.merge(data[[\"eid\", time0_col]], how=\"left\", on=\"eid\")\n",
    "    \n",
    "    cens_time_right = datetime.date(2020, 9, 30)\n",
    "\n",
    "    df_interval = diagnoses_codes_time0[(diagnoses_codes_time0.date > diagnoses_codes_time0[time0_col]) & \n",
    "                                        (diagnoses_codes_time0.date < cens_time_right)]\n",
    "    \n",
    "    temp = data[[\"eid\", time0_col]].copy()\n",
    "    for ph, ph_codes in tqdm(endpoint_list.items()):\n",
    "        regex = \"|\".join(ph_codes)\n",
    "        ph_df = df_interval[df_interval.meaning.str.contains(regex, case=False)] \\\n",
    "            .sort_values('date').groupby('eid').head(1).assign(phenotype=1, date=lambda x: x.date)\n",
    "        temp_ph = temp.merge(ph_df, how=\"left\", on=\"eid\").fillna(0)\n",
    "        temp[ph+\"_event\"], temp[ph+\"_event_date\"] = temp_ph.phenotype, temp_ph.date\n",
    "        \n",
    "        fill_date = {ph+\"_event_date\" : lambda x: [cens_time_right if event==0 else event_date for event, event_date in zip(x[ph+\"_event\"], x[ph+\"_event_date\"])]}\n",
    "        calc_tte = {ph+\"_event_time\" : lambda x: [(event_date-time0).days/365.25  for time0, event_date in zip(x[time0_col], x[ph+\"_event_date\"])]}\n",
    "        \n",
    "        temp = temp.assign(**fill_date).assign(**calc_tte).drop([ph+\"_event_date\"], axis=1)\n",
    "        \n",
    "    temp = temp.drop([time0_col], axis=1)     \n",
    "    \n",
    "    return temp.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basics = pd.read_feather(os.path.join(path, dataset_path, 'temp_basics.feather'))\n",
    "endpoints_diagnoses = extract_endpoints_tte(basics, endpoint_codes, endpoint_list, time0_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Death registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_list = {\n",
    "    \"death_allcause\":[],\n",
    "    \"death_cvd\":['I{:02}'.format(ID+1) for ID in range(0, 98)],\n",
    "}\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'death_list.yaml'), 'w') as file:\n",
    "    yaml.dump(death_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_death = extract_endpoints_tte(basics, death_codes, death_list, time0_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list = {\n",
    "    \"SCORE\":['I{:02}'.format(ID) for ID in [10, 11, 12, 13, 14, 15, 20, 21, 22, 23, 24, 25, 44, 45, 46, 47, 48, 49, 50, 51, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]],\n",
    "    \"ASCVD\":['I{:02}'.format(ID) for ID in [20, 21, 22, 23, 24, 25, 63]],\n",
    "    \"QRISK3\":[\"G45\", \"I20\", \"I21\", \"I22\", \"I23\", \"I24\", \"I25\", \"I63\", \"I64\"],\n",
    "    \"MACE\":[\"G45\", \"I21\", \"I22\", \"I23\", \"I24\", \"I25\", \"I63\", \"I64\"],    \n",
    "}\n",
    "with open(os.path.join(path, dataset_path, 'scores_list.yaml'), 'w') as file:\n",
    "    yaml.dump(scores_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_scores =  extract_endpoints_tte(basics, death_codes, scores_list, time0_col=time0_col)\n",
    "endpoint_scores = extract_endpoints_tte(basics, endpoint_codes, scores_list, time0_col=time0_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints_scores_all = death_scores[[\"eid\", \"SCORE_event\", \"SCORE_event_time\"]].merge(endpoint_scores[[\"eid\", \"ASCVD_event\", \"ASCVD_event_time\", \"QRISK3_event\", \"QRISK3_event_time\", \"MACE_event\", \"MACE_event_time\"]], on=\"eid\")\n",
    "endpoints_scores_all.to_feather(os.path.join(path, dataset_path, 'temp_endpoints_scores_all.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs_dict = {\"endpoints_diagnoses\":endpoints_diagnoses, \n",
    "                 \"endpoints_death\":endpoints_death, \n",
    "                 \"endpoints_scores_all\":endpoints_scores_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_clean(df):\n",
    "    df.columns = df.columns.str.replace(r'_0_0$', '').str.replace(r'_f[0-9]+$', '').str.replace(\"_automated_reading\", '')\n",
    "    return df.columns\n",
    "\n",
    "def clean_df(df):\n",
    "    df.columns = get_cols_clean(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "data_baseline = reduce(lambda x, y: pd.merge(x, y, on = 'eid'), list(data_dfs_dict.values()))\n",
    "endpoint_columns = [c[:-11] for c in data_baseline.columns.tolist() if \"_event_time\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_baseline = clean_df(data_baseline)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in [col for col in list(data_baseline.columns) if (\"_event\" in col) & (\"_time\" not in col)]:\n",
    "    data_baseline[col] = data_baseline[col].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covariates = [col for col in list(data_baseline.columns) if not \"_event\" in col]\n",
    "targets = [col for col in list(data_baseline.columns) if \"_event\" in col]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = {}\n",
    "for topic, df in data_dfs_dict.items(): \n",
    "    data_cols[\"eid\"] = [\"admin\"]\n",
    "    data_cols[topic]=list(get_cols_clean(df))[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_single = {}\n",
    "for topic, columns in data_cols.items():\n",
    "    for col in columns:\n",
    "        data_cols_single[col] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [c for c in data_baseline.columns.tolist() if \"comp\" in c]:\n",
    "    data_cols_single.update({c:\"endpoints_competing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"int32\":\"int\", \"int64\":\"int\", \"float64\":\"float\", \"category\":\"category\", \"object\":\"category\", \"bool\":\"bool\"}\n",
    "desc_dict = {\"id\": [*range(1, len(data_baseline.columns.to_list())+1)] , \n",
    "             \"covariate\": data_baseline.columns.to_list(), \n",
    "             \"dtype\":[dtypes[str(col)] for col in data_baseline.dtypes.to_list()], \n",
    "             \"isTarget\":[True if col in targets else False for col in data_baseline.columns.to_list()],\n",
    "            \"based_on\":[topic for col, topic in data_cols_single.items()],\n",
    "             \"field\": [np.nan for col in data_baseline.columns.to_list()],\n",
    "            \"aggr_fn\": [np.nan for col in data_baseline.columns.to_list()]}\n",
    "data_baseline_description = pd.DataFrame.from_dict(desc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_dict = {}\n",
    "for group in data_baseline_description.based_on.unique(): endpoint_dict[group] = data_baseline_description.query(\"based_on==@group\").covariate.to_list()\n",
    "with open(os.path.join(path, dataset_path, 'endpoint_list.yaml'), 'w') as file: yaml.dump(endpoint_dict, file, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE FEATURES IN YAML!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline.to_feather(os.path.join(path, dataset_path, 'baseline_endpoints.feather'))\n",
    "data_baseline_description.to_feather(os.path.join(path, dataset_path, 'baseline_endpoints_description.feather'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pl1.x]",
   "language": "python",
   "name": "conda-env-miniconda3-pl1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}