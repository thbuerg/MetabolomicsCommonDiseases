{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.436340Z",
     "start_time": "2020-11-04T12:31:48.732042Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_name = \"name_of_your_dataset\"\n",
    "path = \"/path/to/mapping/files\"\n",
    "data_path = \"/path/to/decoded/output\"\n",
    "dataset_path = f\"{data_path}/2_datasets_pre/{dataset_name}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:31:49.895222Z",
     "start_time": "2020-11-04T12:31:49.891332Z"
    }
   },
   "outputs": [],
   "source": [
    "Path(dataset_path).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_feather(f\"{data_path}/1_decoded/ukb_data.feather\")\n",
    "data_field = pd.read_feather(f\"{data_path}/1_decoded/ukb_data_field.feather\")\n",
    "data_columns = data.columns.to_list()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings + Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.867152Z",
     "start_time": "2020-11-04T12:33:16.878773Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop obvious missing data\n",
    "print(len(data))\n",
    "data = data.dropna(subset=[\"sex_f31_0_0\"], axis=0)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.872216Z",
     "start_time": "2020-11-04T12:34:05.869505Z"
    }
   },
   "outputs": [],
   "source": [
    "time0_col=\"date_of_attending_assessment_centre_f53_0_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:05.889725Z",
     "start_time": "2020-11-04T12:34:05.874587Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_fields(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields) & data_field[\"field.tab\"].str.contains(\"f\\\\.\\\\d+\\\\.0\\\\.\\\\d\")].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_fields_all(fields, data, data_field):\n",
    "    f = data_field[data_field[\"field.showcase\"].isin(fields)].copy()\n",
    "    f[\"field\"] = pd.Categorical(f[\"field.showcase\"], categories=fields, ordered=True)\n",
    "    f = f.sort_values(\"field\").reset_index().drop(\"field\", axis=1)\n",
    "    return f\n",
    "\n",
    "def get_data_fields(fields, data, data_field):\n",
    "    f = get_fields(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()\n",
    "\n",
    "def get_data_fields_all(fields, data, data_field):\n",
    "    f = get_fields_all(fields, data, data_field)\n",
    "    return data[[\"eid\"]+f[\"col.name\"].to_list()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding10 = pd.read_csv(f\"{data_path}/mapping/codings/coding10.tsv\", sep=\"\\t\").assign(coding = lambda x: x.coding.astype(\"int\")).rename(columns={\"coding\":\"uk_biobank_assessment_centre_f54_0_0\"})\n",
    "coding10[\"uk_biobank_assessment_centre_f54_0_0\"] = coding10[\"uk_biobank_assessment_centre_f54_0_0\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:06.176411Z",
     "start_time": "2020-11-04T12:34:05.891730Z"
    }
   },
   "outputs": [],
   "source": [
    "fields_basics = [\n",
    "    \"21022\", # age at recruitment\n",
    "    \"31\", # sex\n",
    "    \"21000\", # ethnicity\n",
    "    \"189\", # Townsend index\n",
    "    \"53\", # date of baseline assessment\n",
    "    \"54\", # assessment center\n",
    "]\n",
    "\n",
    "temp = get_data_fields(fields_basics, data, data_field)\n",
    "\n",
    "temp[\"sex_f31_0_0\"] = temp[\"sex_f31_0_0\"].cat.set_categories([\"Female\", 'Male'], ordered=False)\n",
    "\n",
    "ethn_bg_def = {\"White\": [\"White\", \"British\", \"Irish\", \"Any other white background\"],\n",
    "                \"Mixed\": [\"Mixed\", \"White and Black Caribbean\", \"White and Black African\", \"White and Asian\", \"Any other mixed background\"],  \n",
    "                \"Asian\": [\"Asian or Asian British\", \"Indian\", \"Pakistani\", \"Bangladeshi\", \"Any other Asian background\"], \n",
    "                \"Black\": [\"Black or Black British\", \"Caribbean\", \"African\", \"Any other Black background\"],\n",
    "                \"Chinese\": [\"Chinese\"],  \n",
    "                np.nan: [\"Other ethnic group\", \"Do not know\", \"Prefer not to answer\"]}\n",
    "\n",
    "ethn_bg_dict = {}\n",
    "for key, values in ethn_bg_def.items(): \n",
    "    for value in values:\n",
    "        ethn_bg_dict[value]=key \n",
    "        \n",
    "temp[\"ethnic_background_f21000_0_0\"].replace(ethn_bg_dict, inplace=True)\n",
    "temp[\"ethnic_background_f21000_0_0\"] = temp[\"ethnic_background_f21000_0_0\"].astype(\"category\")\n",
    "\n",
    "basics = temp\n",
    "\n",
    "calc_birth_date = [date_of_attending_assessment_centre - relativedelta(years=age_at_recruitment)\n",
    "                                                             for date_of_attending_assessment_centre, age_at_recruitment \n",
    "                                                             in zip(basics[\"date_of_attending_assessment_centre_f53_0_0\"], basics[\"age_at_recruitment_f21022_0_0\"])]\n",
    "\n",
    "basics = basics.assign(birth_date = calc_birth_date)\n",
    "basics[\"uk_biobank_assessment_centre_f54_0_0\"] = basics.assign(uk_biobank_assessment_centre_f54_0_0 = lambda x: x.uk_biobank_assessment_centre_f54_0_0.astype(\"int\")).merge(coding10, on=\"uk_biobank_assessment_centre_f54_0_0\")[\"meaning\"]\n",
    "\n",
    "basics.to_feather(os.path.join(path, dataset_path, 'temp_basics.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:06.389467Z",
     "start_time": "2020-11-04T12:34:06.188206Z"
    }
   },
   "outputs": [],
   "source": [
    "fields_questionnaire = [\n",
    "    \"2178\", # Overall health\n",
    "    \"20116\", # Smoking status\n",
    "    \"1558\",\n",
    "]\n",
    "\n",
    "temp = get_data_fields(fields_questionnaire, data, data_field)\n",
    "\n",
    "temp[\"overall_health_rating_f2178_0_0\"] = temp[\"overall_health_rating_f2178_0_0\"]\\\n",
    "    .replace({\"Do not know\": np.nan, \"Prefer not to answer\": np.nan})\\\n",
    "    .astype(\"category\").cat.set_categories(['Poor', 'Fair', 'Good', 'Excellent'], ordered=True)\n",
    "\n",
    "\n",
    "temp[\"smoking_status_f20116_0_0\"] = temp[\"smoking_status_f20116_0_0\"]\\\n",
    "    .replace({\"Prefer not to answer\": np.nan}, inplace=False)\\\n",
    "    .astype(\"category\").cat.set_categories(['Current', 'Previous', 'Never'], ordered=True)\n",
    "\n",
    "temp[\"alcohol_intake_frequency_f1558_0_0\"] = temp[\"alcohol_intake_frequency_f1558_0_0\"]\\\n",
    "    .replace({\"Prefer not to answer\": np.nan}, inplace=False)\\\n",
    "    .astype(\"category\").cat.set_categories([\n",
    "        'Daily or almost daily', \n",
    "        'Three or four times a week', \n",
    "        'Once or twice a week',\n",
    "        'One to three times a month',\n",
    "        'Special occasions only', \n",
    "        'Never'], ordered=True)\n",
    "\n",
    "questionnaire = temp\n",
    "\n",
    "questionnaire.to_feather(os.path.join(path, dataset_path, 'temp_questionnaire.feather'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:07.052989Z",
     "start_time": "2020-11-04T12:34:06.400858Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fields_measurements = [\n",
    "    \"21001\", # BMI\n",
    "    \"21002\", # weight\n",
    "    \"4080\", # Syst. BP\n",
    "    \"4079\", # Diast. BP\n",
    "    \"102\",\n",
    "    \"21021\",\n",
    "    \"4195\",\n",
    "    \"48\",\n",
    "    \"49\",\n",
    "    \"50\",\n",
    "    \"23127\",\n",
    "    \"23099\",\n",
    "    \"23105\",\n",
    "    \"20151\",\n",
    "    \"20150\",\n",
    "    \"20258\",\n",
    "    \"3064\",\n",
    "    \n",
    "]\n",
    "temp = get_data_fields(fields_measurements, data, data_field)\n",
    "\n",
    "sbp_cols = [\"systolic_blood_pressure_automated_reading_f4080_0_0\", \"systolic_blood_pressure_automated_reading_f4080_0_1\"]\n",
    "dbp_cols = [\"diastolic_blood_pressure_automated_reading_f4079_0_0\", \"diastolic_blood_pressure_automated_reading_f4079_0_1\"]\n",
    "pr_cols = [\"pulse_rate_automated_reading_f102_0_0\", \"pulse_rate_automated_reading_f102_0_1\"]\n",
    "\n",
    "temp = temp.assign(systolic_blood_pressure_automated_reading_f4080 = temp[sbp_cols].mean(axis=1),\n",
    "                   diastolic_blood_pressure_automated_reading_f4079 = temp[dbp_cols].mean(axis=1),\n",
    "                   pulse_rate_automated_reading_f102 = temp[pr_cols].mean(axis=1))\\\n",
    "    .drop(sbp_cols + dbp_cols + pr_cols, axis=1)\n",
    "\n",
    "measurements = temp\n",
    "\n",
    "measurements.to_feather(os.path.join(path, dataset_path, 'temp_measurements.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:07.647955Z",
     "start_time": "2020-11-04T12:34:07.055242Z"
    }
   },
   "outputs": [],
   "source": [
    "fields_blood_count = [\n",
    "    \"30160\", #\tBasophill count\n",
    "    \"30220\", #\tBasophill percentage\n",
    "    \"30150\", #\tEosinophill count\n",
    "    \"30210\", #\tEosinophill percentage\n",
    "    \"30030\", #\tHaematocrit percentage\n",
    "    \"30020\", #\tHaemoglobin concentration\n",
    "    \"30300\", #\tHigh light scatter reticulocyte count\n",
    "    \"30290\", #\tHigh light scatter reticulocyte percentage\n",
    "    \"30280\", #\tImmature reticulocyte fraction\n",
    "    \"30120\", #\tLymphocyte count\n",
    "    \"30180\", #\tLymphocyte percentage\n",
    "    \"30050\", #\tMean corpuscular haemoglobin\n",
    "    \"30060\", #\tMean corpuscular haemoglobin concentration\n",
    "    \"30040\", #\tMean corpuscular volume\n",
    "    \"30100\", #\tMean platelet (thrombocyte) volume\n",
    "    \"30260\", #\tMean reticulocyte volume\n",
    "    \"30270\", #\tMean sphered cell volume\n",
    "    \"30130\", #\tMonocyte count\n",
    "    \"30190\", #\tMonocyte percentage\n",
    "    \"30140\", #\tNeutrophill count\n",
    "    \"30200\", #\tNeutrophill percentage\n",
    "    \"30170\", #\tNucleated red blood cell count\n",
    "    \"30230\", #\tNucleated red blood cell percentage\n",
    "    \"30080\", #\tPlatelet count\n",
    "    \"30090\", #\tPlatelet crit\n",
    "    \"30110\", #\tPlatelet distribution width\n",
    "    \"30010\", #\tRed blood cell (erythrocyte) count\n",
    "    \"30070\", #\tRed blood cell (erythrocyte) distribution width\n",
    "    \"30250\", #\tReticulocyte count\n",
    "    \"30240\", #\tReticulocyte percentage\n",
    "    \"30000\", #\tWhite blood cell (leukocyte) count\n",
    "]\n",
    "\n",
    "fields_blood_biochemistry = [\n",
    "    \"30620\",#\tAlanine aminotransferase\n",
    "    \"30600\",#\tAlbumin\n",
    "    \"30610\",#\tAlkaline phosphatase\n",
    "    \"30630\",#\tApolipoprotein A\n",
    "    \"30640\",#\tApolipoprotein B\n",
    "    \"30650\",#\tAspartate aminotransferase\n",
    "    \"30710\",#\tC-reactive protein\n",
    "    \"30680\",#\tCalcium\n",
    "    \"30690\",#\tCholesterol\n",
    "    \"30700\",#\tCreatinine\n",
    "    \"30720\",#\tCystatin C\n",
    "    \"30660\",#\tDirect bilirubin\n",
    "    \"30730\",#\tGamma glutamyltransferase\n",
    "    \"30740\",#\tGlucose\n",
    "    \"30750\",#\tGlycated haemoglobin (HbA1c)\n",
    "    \"30760\",#\tHDL cholesterol\n",
    "    \"30770\",#\tIGF-1\n",
    "    \"30780\",#\tLDL direct\n",
    "    \"30790\",#\tLipoprotein A\n",
    "    \"30800\",#\tOestradiol\n",
    "    \"30810\",#\tPhosphate\n",
    "    \"30820\",#\tRheumatoid factor\n",
    "    \"30830\",#\tSHBG\n",
    "    \"30850\",#\tTestosterone\n",
    "    \"30840\",#\tTotal bilirubin\n",
    "    \"30860\",#\tTotal protein\n",
    "    \"30870\",#\tTriglycerides\n",
    "    \"30880\",#\tUrate\n",
    "    \"30670\",#\tUrea\n",
    "    \"30890\",#\tVitamin D\n",
    "]\n",
    "\n",
    "fields_blood_infectious = [\n",
    "    \"23000\", #\t1gG antigen for Herpes Simplex virus-1\n",
    "    \"23001\", #\t2mgG unique antigen for Herpes Simplex virus-2\n",
    "    \"23049\", #\tAntigen assay QC indicator\n",
    "    \"23048\", #\tAntigen assay date\n",
    "    \"23026\", #\tBK VP1 antigen for Human Polyomavirus BKV\n",
    "    \"23039\", #\tCagA antigen for Helicobacter pylori\n",
    "    \"23043\", #\tCatalase antigen for Helicobacter pylori\n",
    "    \"23018\", #\tCore antigen for Hepatitis C Virus\n",
    "    \"23030\", #\tE6 antigen for Human Papillomavirus type-16\n",
    "    \"23031\", #\tE7 antigen for Human Papillomavirus type-16\n",
    "    \"23006\", #\tEA-D antigen for Epstein-Barr Virus\n",
    "    \"23004\", #\tEBNA-1 antigen for Epstein-Barr Virus\n",
    "    \"23042\", #\tGroEL antigen for Helicobacter pylori\n",
    "    \"23016\", #\tHBc antigen for Hepatitis B Virus\n",
    "    \"23017\", #\tHBe antigen for Hepatitis B Virus\n",
    "    \"23025\", #\tHIV-1 env antigen for Human Immunodeficiency Virus\n",
    "    \"23024\", #\tHIV-1 gag antigen for Human Immunodeficiency Virus\n",
    "    \"23023\", #\tHTLV-1 env antigen for Human T-Lymphotropic Virus 1\n",
    "    \"23022\", #\tHTLV-1 gag antigen for Human T-Lymphotropic Virus 1\n",
    "    \"23010\", #\tIE1A antigen for Human Herpesvirus-6\n",
    "    \"23011\", #\tIE1B antigen for Human Herpesvirus-6\n",
    "    \"23027\", #\tJC VP1 antigen for Human Polyomavirus JCV\n",
    "    \"23015\", #\tK8.1 antigen for Kaposi's Sarcoma-Associated Herpesvirus\n",
    "    \"23029\", #\tL1 antigen for Human Papillomavirus type-16\n",
    "    \"23032\", #\tL1 antigen for Human Papillomavirus type-18\n",
    "    \"23014\", #\tLANA antigen for Kaposi's Sarcoma-Associated Herpesvirus\n",
    "    \"23028\", #\tMC VP1 antigen for Merkel Cell Polyomavirus\n",
    "    \"23019\", #\tNS3 antigen for Hepatitis C Virus\n",
    "    \"23041\", #\tOMP antigen for Helicobacter pylori\n",
    "    \"23037\", #\tPorB antigen for Chlamydia trachomatis\n",
    "    \"23013\", #\tU14 antigen for Human Herpesvirus-7\n",
    "    \"23044\", #\tUreA antigen for Helicobacter pylori\n",
    "    \"23003\", #\tVCA p18 antigen for Epstein-Barr Virus\n",
    "    \"23040\", #\tVacA antigen for Helicobacter pylori\n",
    "    \"23005\", #\tZEBRA antigen for Epstein-Barr Virus\n",
    "    \"23002\", #\tgE / gI antigen for Varicella Zoster Virus\n",
    "    \"23034\", #\tmomp A antigen for Chlamydia trachomatis\n",
    "    \"23033\", #\tmomp D antigen for Chlamydia trachomatis\n",
    "    \"23012\", #\tp101 k antigen for Human Herpesvirus-6\n",
    "    \"23020\", #\tp22 antigen for Toxoplasma gondii\n",
    "    \"23038\", #\tpGP3 antigen for Chlamydia trachomatis\n",
    "    \"23009\", #\tpp 28 antigen for Human Cytomegalovirus\n",
    "    \"23008\", #\tpp 52 antigen for Human Cytomegalovirus\n",
    "    \"23007\", #\tpp150 Nter antigen for Human Cytomegalovirus\n",
    "    \"23021\", #\tsag1 antigen for Toxoplasma gondii\n",
    "    \"23035\", #\ttarp-D F1 antigen for Chlamydia trachomatis\n",
    "    \"23036\", #\ttarp-D F2 antigen for Chlamydia trachomatis\n",
    "]\n",
    "\n",
    "labs = temp = get_data_fields(fields_blood_count+fields_blood_biochemistry+fields_blood_infectious, data, data_field)\n",
    "\n",
    "labs.to_feather(os.path.join(path, dataset_path, 'temp_labs.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_metabolomics = [\n",
    "\"23474\", #3-Hydroxybutyrate\n",
    "\"23475\", #Acetate\n",
    "\"23476\", #Acetoacetate\n",
    "\"23477\", #Acetone\n",
    "\"23460\", #Alanine\n",
    "\"23479\", #Albumin\n",
    "\"23440\", #Apolipoprotein A1\n",
    "\"23439\", #Apolipoprotein B\n",
    "\"23433\", #Average Diameter for HDL Particles\n",
    "\"23432\", #Average Diameter for LDL Particles\n",
    "\"23431\", #Average Diameter for VLDL Particles\n",
    "\"23484\", #Cholesterol in Chylomicrons and Extremely Large VLDL\n",
    "\"23526\", #Cholesterol in IDL\n",
    "\"23561\", #Cholesterol in Large HDL\n",
    "\"23533\", #Cholesterol in Large LDL\n",
    "\"23498\", #Cholesterol in Large VLDL\n",
    "\"23568\", #Cholesterol in Medium HDL\n",
    "\"23540\", #Cholesterol in Medium LDL\n",
    "\"23505\", #Cholesterol in Medium VLDL\n",
    "\"23575\", #Cholesterol in Small HDL\n",
    "\"23547\", #Cholesterol in Small LDL\n",
    "\"23512\", #Cholesterol in Small VLDL\n",
    "\"23554\", #Cholesterol in Very Large HDL\n",
    "\"23491\", #Cholesterol in Very Large VLDL\n",
    "\"23519\", #Cholesterol in Very Small VLDL\n",
    "\"23485\", #Cholesteryl Esters in Chylomicrons and Extremely Large VLDL\n",
    "\"23418\", #Cholesteryl Esters in HDL\n",
    "\"23527\", #Cholesteryl Esters in IDL\n",
    "\"23417\", #Cholesteryl Esters in LDL\n",
    "\"23562\", #Cholesteryl Esters in Large HDL\n",
    "\"23534\", #Cholesteryl Esters in Large LDL\n",
    "\"23499\", #Cholesteryl Esters in Large VLDL\n",
    "\"23569\", #Cholesteryl Esters in Medium HDL\n",
    "\"23541\", #Cholesteryl Esters in Medium LDL\n",
    "\"23506\", #Cholesteryl Esters in Medium VLDL\n",
    "\"23576\", #Cholesteryl Esters in Small HDL\n",
    "\"23548\", #Cholesteryl Esters in Small LDL\n",
    "\"23513\", #Cholesteryl Esters in Small VLDL\n",
    "\"23416\", #Cholesteryl Esters in VLDL\n",
    "\"23555\", #Cholesteryl Esters in Very Large HDL\n",
    "\"23492\", #Cholesteryl Esters in Very Large VLDL\n",
    "\"23520\", #Cholesteryl Esters in Very Small VLDL\n",
    "\"23473\", #Citrate\n",
    "\"23404\", #Clinical LDL Cholesterol\n",
    "\"23481\", #Concentration of Chylomicrons and Extremely Large VLDL Particles\n",
    "\"23430\", #Concentration of HDL Particles\n",
    "\"23523\", #Concentration of IDL Particles\n",
    "\"23429\", #Concentration of LDL Particles\n",
    "\"23558\", #Concentration of Large HDL Particles\n",
    "\"23530\", #Concentration of Large LDL Particles\n",
    "\"23495\", #Concentration of Large VLDL Particles\n",
    "\"23565\", #Concentration of Medium HDL Particles\n",
    "\"23537\", #Concentration of Medium LDL Particles\n",
    "\"23502\", #Concentration of Medium VLDL Particles\n",
    "\"23572\", #Concentration of Small HDL Particles\n",
    "\"23544\", #Concentration of Small LDL Particles\n",
    "\"23509\", #Concentration of Small VLDL Particles\n",
    "\"23428\", #Concentration of VLDL Particles\n",
    "\"23551\", #Concentration of Very Large HDL Particles\n",
    "\"23488\", #Concentration of Very Large VLDL Particles\n",
    "\"23516\", #Concentration of Very Small VLDL Particles\n",
    "\"23478\", #Creatinine\n",
    "\"23443\", #Degree of Unsaturation\n",
    "\"23450\", #Docosahexaenoic Acid\n",
    "\"23486\", #Free Cholesterol in Chylomicrons and Extremely Large VLDL\n",
    "\"23422\", #Free Cholesterol in HDL\n",
    "\"23528\", #Free Cholesterol in IDL\n",
    "\"23421\", #Free Cholesterol in LDL\n",
    "\"23563\", #Free Cholesterol in Large HDL\n",
    "\"23535\", #Free Cholesterol in Large LDL\n",
    "\"23500\", #Free Cholesterol in Large VLDL\n",
    "\"23570\", #Free Cholesterol in Medium HDL\n",
    "\"23542\", #Free Cholesterol in Medium LDL\n",
    "\"23507\", #Free Cholesterol in Medium VLDL\n",
    "\"23577\", #Free Cholesterol in Small HDL\n",
    "\"23549\", #Free Cholesterol in Small LDL\n",
    "\"23514\", #Free Cholesterol in Small VLDL\n",
    "\"23420\", #Free Cholesterol in VLDL\n",
    "\"23556\", #Free Cholesterol in Very Large HDL\n",
    "\"23493\", #Free Cholesterol in Very Large VLDL\n",
    "\"23521\", #Free Cholesterol in Very Small VLDL\n",
    "\"23470\", #Glucose\n",
    "\"23461\", #Glutamine\n",
    "\"23462\", #Glycine\n",
    "\"23480\", #Glycoprotein Acetyls\n",
    "\"23406\", #HDL Cholesterol\n",
    "\"23463\", #Histidine\n",
    "\"23465\", #Isoleucine\n",
    "\"23405\", #LDL Cholesterol\n",
    "\"23471\", #Lactate\n",
    "\"23466\", #Leucine\n",
    "\"23449\", #Linoleic Acid\n",
    "\"23447\", #Monounsaturated Fatty Acids\n",
    "\"23444\", #Omega-3 Fatty Acids\n",
    "\"23445\", #Omega-6 Fatty Acids\n",
    "\"23468\", #Phenylalanine\n",
    "\"23437\", #Phosphatidylcholines\n",
    "\"23434\", #Phosphoglycerides\n",
    "\"23483\", #Phospholipids in Chylomicrons and Extremely Large VLDL\n",
    "\"23414\", #Phospholipids in HDL\n",
    "\"23525\", #Phospholipids in IDL\n",
    "\"23413\", #Phospholipids in LDL\n",
    "\"23560\", #Phospholipids in Large HDL\n",
    "\"23532\", #Phospholipids in Large LDL\n",
    "\"23497\", #Phospholipids in Large VLDL\n",
    "\"23567\", #Phospholipids in Medium HDL\n",
    "\"23539\", #Phospholipids in Medium LDL\n",
    "\"23504\", #Phospholipids in Medium VLDL\n",
    "\"23574\", #Phospholipids in Small HDL\n",
    "\"23546\", #Phospholipids in Small LDL\n",
    "\"23511\", #Phospholipids in Small VLDL\n",
    "\"23412\", #Phospholipids in VLDL\n",
    "\"23553\", #Phospholipids in Very Large HDL\n",
    "\"23490\", #Phospholipids in Very Large VLDL\n",
    "\"23518\", #Phospholipids in Very Small VLDL\n",
    "\"23446\", #Polyunsaturated Fatty Acids\n",
    "\"23472\", #Pyruvate\n",
    "\"23402\", #Remnant Cholesterol (Non-HDL, Non-LDL -Cholesterol)\n",
    "\"23448\", #Saturated Fatty Acids\n",
    "\"23438\", #Sphingomyelins\n",
    "\"23400\", #Total Cholesterol\n",
    "\"23401\", #Total Cholesterol Minus HDL-C\n",
    "\"23436\", #Total Cholines\n",
    "\"23464\", #Total Concentration of Branched-Chain Amino Acids (Leucine + Isoleucine + Valine)\n",
    "\"23427\", #Total Concentration of Lipoprotein Particles\n",
    "\"23415\", #Total Esterified Cholesterol\n",
    "\"23442\", #Total Fatty Acids\n",
    "\"23419\", #Total Free Cholesterol\n",
    "\"23482\", #Total Lipids in Chylomicrons and Extremely Large VLDL\n",
    "\"23426\", #Total Lipids in HDL\n",
    "\"23524\", #Total Lipids in IDL\n",
    "\"23425\", #Total Lipids in LDL\n",
    "\"23559\", #Total Lipids in Large HDL\n",
    "\"23531\", #Total Lipids in Large LDL\n",
    "\"23496\", #Total Lipids in Large VLDL\n",
    "\"23423\", #Total Lipids in Lipoprotein Particles\n",
    "\"23566\", #Total Lipids in Medium HDL\n",
    "\"23538\", #Total Lipids in Medium LDL\n",
    "\"23503\", #Total Lipids in Medium VLDL\n",
    "\"23573\", #Total Lipids in Small HDL\n",
    "\"23545\", #Total Lipids in Small LDL\n",
    "\"23510\", #Total Lipids in Small VLDL\n",
    "\"23424\", #Total Lipids in VLDL\n",
    "\"23552\", #Total Lipids in Very Large HDL\n",
    "\"23489\", #Total Lipids in Very Large VLDL\n",
    "\"23517\", #Total Lipids in Very Small VLDL\n",
    "\"23411\", #Total Phospholipids in Lipoprotein Particles\n",
    "\"23407\", #Total Triglycerides\n",
    "\"23487\", #Triglycerides in Chylomicrons and Extremely Large VLDL\n",
    "\"23410\", #Triglycerides in HDL\n",
    "\"23529\", #Triglycerides in IDL\n",
    "\"23409\", #Triglycerides in LDL\n",
    "\"23564\", #Triglycerides in Large HDL\n",
    "\"23536\", #Triglycerides in Large LDL\n",
    "\"23501\", #Triglycerides in Large VLDL\n",
    "\"23571\", #Triglycerides in Medium HDL\n",
    "\"23543\", #Triglycerides in Medium LDL\n",
    "\"23508\", #Triglycerides in Medium VLDL\n",
    "\"23578\", #Triglycerides in Small HDL\n",
    "\"23550\", #Triglycerides in Small LDL\n",
    "\"23515\", #Triglycerides in Small VLDL\n",
    "\"23408\", #Triglycerides in VLDL\n",
    "\"23557\", #Triglycerides in Very Large HDL\n",
    "\"23494\", #Triglycerides in Very Large VLDL\n",
    "\"23522\", #Triglycerides in Very Small VLDL\n",
    "\"23469\", #Tyrosine\n",
    "\"23403\", #VLDL Cholesterol\n",
    "\"23467\", #Valine\n",
    "    \"23651\", \"23650\"\n",
    "]\n",
    "\n",
    "metabolomics = temp = get_data_fields(fields_metabolomics, data, data_field)\n",
    "metabolomics.columns = [\"eid\"]+[f\"NMR_{col}\" for col in metabolomics.columns[1:]]\n",
    "metabolomics[\"NMR_FLAG\"] = [True if type(spectrometer)==str else False for spectrometer in metabolomics[\"NMR_spectrometer_f23650_0_0\"].tolist()]\n",
    "\n",
    "\n",
    "metabolomics.to_feather(os.path.join(path, dataset_path, 'temp_metabolomics.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Family History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:25.838958Z",
     "start_time": "2020-11-04T12:34:07.649920Z"
    }
   },
   "outputs": [],
   "source": [
    "fh_list=[\"Heart disease\", \"Stroke\", \"High blood pressure\",  \"Diabetes\", \"Lung cancer\", \"Severe depression\", \"Parkinson's disease\", \"Alzheimer's disease/dementia\", \"Chronic bronchitis/emphysema\", \"Breast cancer\", \"Bowel cancer\"]\n",
    "with open(os.path.join(path, dataset_path, 'fh_list.yaml'), 'w') as file: yaml.dump(fh_list, file, default_flow_style=False)\n",
    "\n",
    "fields_family_history = [\n",
    "    \"20107\", # Family history \n",
    "    \"20110\" # Family history\n",
    "]\n",
    "\n",
    "raw = get_data_fields(fields_family_history, data, data_field)\n",
    "temp = pd.melt(raw, id_vars=[\"eid\"], value_vars=raw.drop(\"eid\", axis=1).columns.to_list(), var_name = \"field\", value_name=\"family_history\").drop(\"field\", axis=1)\n",
    "temp = temp[temp.family_history.isin(fh_list)].assign(family_history=temp[\"family_history\"].str.lower().replace(\" \", \"_\", regex=True))\n",
    "\n",
    "temp = temp.drop_duplicates().sort_values(\"eid\").reset_index().drop(\"index\", axis=1).assign(n=True)\n",
    "temp = pd.pivot_table(temp, index=\"eid\", columns=\"family_history\", values=\"n\", observed=True).add_prefix('fh_')\n",
    "family_history = temp = data[[\"eid\"]].copy().merge(temp, how=\"left\", on=\"eid\").fillna(False)\n",
    "\n",
    "\n",
    "family_history.to_feather(os.path.join(path, dataset_path, 'temp_family_history.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:25.842601Z",
     "start_time": "2020-11-04T12:34:25.840607Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://list.essentialmeds.org/?showRemoved=0\n",
    "# essential medicines WHO?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:32.471831Z",
     "start_time": "2020-11-04T12:34:25.844339Z"
    }
   },
   "outputs": [],
   "source": [
    "atc_mapping = pd.read_csv(f\"{data_path}/mapping/atc/atc_matched_list.csv\")\n",
    "athena_concepts = pd.read_csv(f\"{data_path}/athena_vocabulary/CONCEPT.csv\", sep=\"\\t\").assign(vocabulary_id = lambda x: x.vocabulary_id.astype(\"string\"), concept_class_id = lambda x: x.concept_class_id.astype(\"string\"))\n",
    "atc_concepts = athena_concepts[athena_concepts.vocabulary_id==\"ATC\"]\n",
    "atc2_concepts = atc_concepts[atc_concepts.concept_class_id==\"ATC 2nd\"].sort_values(\"concept_code\")\n",
    "medication_list = dict(zip([x.lower().replace(\" \", \"_\") for x in atc2_concepts.concept_name.to_list()], [[x] for x in atc2_concepts.concept_code.to_list()]))\n",
    "medication_list_extra = {\n",
    "    \"antihypertensives\": [\"C02\"],\n",
    "    \"statins\": [\"C10A\", \"C10B\"],\n",
    "    \"ass\": [\"B01\"],\n",
    "    \"atypical_antipsychotics\" : [\"N05\"],\n",
    "    \"glucocorticoids\" : [\"H02\"]                        \n",
    "}\n",
    "medication_list.update(medication_list_extra)\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'medication_list.yaml'), 'w') as file: yaml.dump(medication_list, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:34:32.481137Z",
     "start_time": "2020-11-04T12:34:32.473698Z"
    }
   },
   "outputs": [],
   "source": [
    "def had_medication_before(data, data_field, medications, atc_mapping):\n",
    "    fields = [\"20003\"]\n",
    "    raw = get_data_fields(fields, data, data_field)\n",
    "    temp = pd.melt(raw, id_vars=[\"eid\"], value_vars=raw.drop(\"eid\", axis=1).columns.to_list(), var_name = \"field\", value_name=\"UKBB_code\").drop(\"field\", axis=1).drop_duplicates()\n",
    "\n",
    "    temp.UKBB_code = temp.UKBB_code.astype(str)\n",
    "    temp = temp[temp.UKBB_code!=\"None\"].copy()\n",
    "    temp = temp[temp.UKBB_code!=\"nan\"].copy()\n",
    "    temp.UKBB_code = temp.UKBB_code.astype(int)\n",
    "\n",
    "    temp_atc = temp.merge(atc_mapping, how=\"left\", on=\"UKBB_code\").sort_values(\"eid\").reset_index(drop=True).dropna(subset=[\"ATC_code\"], axis=0)\n",
    "    temp_atc.ATC_code = temp_atc.ATC_code.astype(\"string\")\n",
    "    temp = data[[\"eid\"]].copy()\n",
    "    for med, med_codes in tqdm(medication_list.items()):\n",
    "        regex_str = \"^\"+\"|^\".join(med_codes)\n",
    "        df = temp_atc[temp_atc.ATC_code.str.contains(regex_str, case=False)][[\"eid\"]]\\\n",
    "            .drop_duplicates(subset=[\"eid\"])\\\n",
    "            .assign(medication=True)\n",
    "        temp[med] = temp.merge(df, how=\"left\", on=\"eid\").fillna(False).medication\n",
    "        \n",
    "    return temp.sort_values(\"eid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:36:14.425612Z",
     "start_time": "2020-11-04T12:34:32.482863Z"
    }
   },
   "outputs": [],
   "source": [
    "medications = had_medication_before(data, data_field, medication_list, atc_mapping)\n",
    "\n",
    "medications.to_feather(os.path.join(path, dataset_path, 'temp_medications.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnoses and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.667281Z",
     "start_time": "2020-11-04T12:36:14.427693Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_dir = f\"{data_path}/mapping/athena\"\n",
    "vocab = {\n",
    "    \"concept\": pd.read_csv(f\"{vocab_dir}/CONCEPT.csv\", sep='\\t'),\n",
    "    \"domain\": pd.read_csv(f\"{vocab_dir}/DOMAIN.csv\", sep='\\t'),\n",
    "    \"class\": pd.read_csv(f\"{vocab_dir}/CONCEPT_CLASS.csv\", sep='\\t'),\n",
    "    \"relationship\": pd.read_csv(f\"{vocab_dir}/RELATIONSHIP.csv\", sep='\\t'),\n",
    "    \"drug_strength\": pd.read_csv(f\"{vocab_dir}/DRUG_STRENGTH.csv\", sep='\\t'),\n",
    "    \"vocabulary\": pd.read_csv(f\"{vocab_dir}/VOCABULARY.csv\", sep='\\t'),\n",
    "    \"concept_synonym\": pd.read_csv(f\"{vocab_dir}/CONCEPT_SYNONYM.csv\", sep='\\t'),\n",
    "    \"concept_ancestor\": pd.read_csv(f\"{vocab_dir}/CONCEPT_ANCESTOR.csv\", sep='\\t'),\n",
    "    \"concept_relationship\": pd.read_csv(f\"{vocab_dir}/CONCEPT_RELATIONSHIP.csv\", sep='\\t')                       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.772869Z",
     "start_time": "2020-11-04T12:37:14.669541Z"
    }
   },
   "outputs": [],
   "source": [
    "coding1836 = pd.read_csv(f\"{data_path}/mapping/codings/coding1836.tsv\", sep=\"\\t\").rename(columns={\"coding\":\"code\"})\n",
    "phecodes = pd.read_csv(f\"{data_path}/mapping/phecodes/phecode_icd10.csv\")\n",
    "def phenotype_children(phecodes, phenotype_list):\n",
    "    l={}\n",
    "    phecodes = phecodes.dropna(subset=[\"Phenotype\"], axis=0)\n",
    "    for ph, ph_names in phenotype_list.items():\n",
    "        regex = \"|\".join(ph_names)\n",
    "        l[ph] = list(phecodes[phecodes.Phenotype.str.contains(regex, case=False)].ICD10.str.replace(\"\\\\.\", \"\").str.slice(0, 3).unique())\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.851438Z",
     "start_time": "2020-11-04T12:37:14.774599Z"
    }
   },
   "outputs": [],
   "source": [
    "snomed_core = pd.read_csv(f\"{data_path}/mapping/snomed_core_list.txt\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.909023Z",
     "start_time": "2020-11-04T12:37:14.853286Z"
    }
   },
   "outputs": [],
   "source": [
    "snomed_core = snomed_core.query(\"SNOMED_CONCEPT_STATUS == 'Current'\").copy()\n",
    "new = snomed_core.SNOMED_FSN.str.split(\"(\", n=1, expand=True)\n",
    "snomed_core[\"snomed_name\"] = new[0].str.rstrip(' ')\n",
    "snomed_core[\"snomed_type\"] = new[1].str.rstrip(')')\n",
    "snomed_core_data = snomed_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:37:14.914973Z",
     "start_time": "2020-11-04T12:37:14.910857Z"
    }
   },
   "outputs": [],
   "source": [
    "snomed_names = snomed_core_data.snomed_name.to_list()\n",
    "snomed_names = [str(item).lower().strip().replace(\" \", \"_\").replace(\";\", \"\").replace(\",\", \"\") for item in snomed_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotype_list_snomed = dict(zip(snomed_names, snomed_core_data.SNOMED_CID.to_list()))\n",
    "snomed_df = pd.DataFrame.from_dict(phenotype_list_snomed, orient='index').reset_index()\n",
    "snomed_df.columns = [\"diagnosis\", \"concept_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_ids = vocab[\"concept\"].query(\"(vocabulary_id == 'SNOMED') | (vocabulary_id == 'ICD10CM')\")\n",
    "concept_ids_icd10 = vocab[\"concept\"].query(\"vocabulary_id == 'ICD10CM'\").concept_id.to_list()\n",
    "vocab_concept_ids = concept_ids.concept_id.to_list()\n",
    "concept_ancestor = vocab[\"concept_ancestor\"][[\"ancestor_concept_id\", \"descendant_concept_id\"]].query(\"ancestor_concept_id == @vocab_concept_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_rel = vocab[\"concept_relationship\"][[\"concept_id_1\", \"concept_id_2\", \"relationship_id\"]].query(\"(concept_id_1 == @vocab_concept_ids) & (concept_id_2 == @concept_ids_icd10) & (relationship_id == 'Mapped from')\")\n",
    "concept_mapping = concept_rel.rename(columns={\"concept_id_2\":\"concept_id\"}).merge(concept_ids, on=\"concept_id\").query(\"vocabulary_id == 'ICD10CM'\")[[\"concept_id_1\", \"concept_code\"]].rename(columns={\"concept_id_1\":\"concept_id_desc\",\"concept_code\":\"icd10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snomed_concept_id = snomed_df.merge(concept_ids[[\"concept_code\", \"concept_id\"]], on=\"concept_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icd = df_snomed_concept_id.merge(concept_mapping.rename(columns={\"concept_id_desc\":\"concept_id\"}), on=\"concept_id\")\n",
    "df_mapped = df_icd[[\"diagnosis\", \"concept_code\", \"icd10\"]].drop_duplicates()\n",
    "df_mapped[\"icd10\"] = df_mapped[\"icd10\"].str.replace(\".\", \"\")\n",
    "df_mapped[\"meaning\"] = [e[:3] for e in df_mapped[\"icd10\"].to_list()]\n",
    "icd10_codes = dict(df_mapped[[\"diagnosis\", \"meaning\"]].drop_duplicates().groupby(\"diagnosis\")[\"meaning\"].apply(list).to_dict())#set_index(\"diagnosis\", drop=True).to_dict()[\"meaning\"]#.sort_values(\"diagnosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.348009Z",
     "start_time": "2020-11-04T12:39:55.344704Z"
    }
   },
   "outputs": [],
   "source": [
    "l10_snomed = {}\n",
    "for ph in icd10_codes: l10_snomed.update({ph:icd10_codes[ph]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_basic = {\n",
    "    \"myocardial_infarction\": ['I21', 'I22', 'I23', 'I24', 'I25'],\n",
    "    \"stroke\": ['G45', \"I63\", \"I64\"],\n",
    "    \"diabetes1\" : ['E10'],\n",
    "    \"diabetes2\" : ['E11', 'E12', 'E13', 'E14'],\n",
    "    \"chronic_kidney_disease\": [\"I12\", \"N18\", \"N19\"],\n",
    "    'atrial_fibrillation': ['I47', 'I48'],\n",
    "    'migraine': ['G43', 'G44'],\n",
    "    'rheumatoid_arthritis': ['J99', 'M05', 'M06', 'M08', 'M12', 'M13'],\n",
    "    \"systemic_lupus_erythematosus\": ['M32'],\n",
    "    'severe_mental_illness': ['F20', 'F25', 'F30', 'F31', 'F32', 'F33', 'F44'],\n",
    "    \"erectile_dysfunction\" : ['F52', 'N48'],  \n",
    "    \"liver_disease\":[\"K70\", \"K71\", \"K72\", \"K73\", \"K74\", \"K75\", \"K76\", \"K77\"],\n",
    "    \"dementia\":['F00', 'F01', 'F02', 'F03'],\n",
    "    \"copd\": ['J44']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_metabolomics = {\n",
    "    \"M_all_cause_dementia\": [\"F00\", \"F01\", \"F02\", \"F03\", \"G30\", \"G31\"],\n",
    "    \"M_MACE\": [\"G45\", \"I21\", \"I22\", \"I23\", \"I24\", \"I25\", \"I63\", \"I64\"],\n",
    "    \"M_type_2_diabetes\": [\"E10\", \"E11\", \"E12\", \"E13\", \"E14\"],\n",
    "    \"M_liver_disease\": [\"B15\", \"B16\", \"B17\", \"B18\", \"B19\", \"C22\", \"E83\", \"E88\", \"I85\", \n",
    "                          \"K70\", \"K72\", \"K73\", \"K74\", \"K75\", \"K76\", \"R18\", \"Z94\"],\n",
    "    \"M_renal_disease\":  [f\"N{i:02}\" for i in range(20)]+[f\"N{i:02}\" for i in range(25, 30)],\n",
    "    \"M_atrial_fibrillation\": [\"I48\"],\n",
    "    \"M_heart_failure\":[\"I50\"],\n",
    "    \"M_coronary_heart_disease\": [f\"I{i:02}\" for i in range(20, 26)],\n",
    "    \"M_venous_thrombosis\": [\"I80\", \"I81\", \"I82\"],\n",
    "    \"M_cerebral_stroke\":[\"I63\", \"I65\", \"I66\"],\n",
    "    \"M_haemorrhagic_stroke\": [\"I60, I61, I62\"],\n",
    "    \"M_abdominal_aortic_aneurysm\" : [\"I71\"],\n",
    "    \"M_peripheral_arterial_disease\": ['I70', 'I71', 'I72', 'I73', 'I74', 'I75', 'I76', 'I77', 'I78', 'I79'],\n",
    "    \"M_asthma\":[\"J45\", \"J46\"],\n",
    "    \"M_chronic_obstructuve_pulmonary_disease\":[\"J40\", \"J41\", \"J42\", \"J43\", \"J44\", \"J47\"],\n",
    "    \"M_lung_cancer\":[\"C33\", \"C34\"],\n",
    "    \"M_non_melanoma_skin_cancer\":[\"C44\"],\n",
    "    \"M_stomach_cancer\":[\"C16\"],\n",
    "    \"M_oesophagus_cancer\":[\"C15\"],\n",
    "    \"M_colon_cancer\":[\"C18\"],\n",
    "    \"M_rectal_cancer\":[\"C19\", \"C20\"],\n",
    "    \"M_prostate_cancer\":[\"C61\"],\n",
    "    \"M_ovarian_cancer\":[\"C56\", \"C57\"],\n",
    "    \"M_breast_cancer\":[\"C50\"],\n",
    "    \"M_uterus_cancer\":[\"C54\"],\n",
    "    \"M_parkinsons_disease\":[\"G20\", \"G21\", \"G22\"],\n",
    "    \"M_fractures\":[\"S02\", \"S12\", \"S22\", \"S32\", \"S42\", \"S52\", \"S62\", \"S72\", \"S82\", \"S92\", \"T02\", \"T08\", \"T10\"],\n",
    "    \"M_cataracts\":[\"H25\", \"H26\"],\n",
    "    \"M_glaucoma\":[\"H40\"]  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.473556Z",
     "start_time": "2020-11-04T12:39:55.471051Z"
    }
   },
   "outputs": [],
   "source": [
    "l10_all = l10_basic\n",
    "for key, value in l10_metabolomics.items(): \n",
    "    if key not in l10_basic: l10_all[key] = value\n",
    "for key, value in l10_snomed.items(): \n",
    "    if key not in l10_basic: l10_all[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:39:55.620916Z",
     "start_time": "2020-11-04T12:39:55.475137Z"
    }
   },
   "outputs": [],
   "source": [
    "l10 = {k: v for k, v in l10_all.items() if len(v)!=0}\n",
    "\n",
    "with open(os.path.join(path, dataset_path, 'phenotype_list.yaml'), 'w') as file: yaml.dump(l10, file, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self Reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:43:13.288198Z",
     "start_time": "2020-11-04T12:43:13.264683Z"
    }
   },
   "outputs": [],
   "source": [
    "coding609 = pd.read_csv(f\"{data_path}/mapping/codings/coding609.tsv\", sep=\"\\t\").rename(columns={\"coding\":\"code\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def datetime_from_dec_year(dec_year):\n",
    "    start = dec_year\n",
    "    year = int(start)\n",
    "    rem = start - year\n",
    "\n",
    "    base = datetime(year, 1, 1)\n",
    "    result = base + timedelta(seconds=(base.replace(year=base.year + 1) - base).total_seconds() * rem)\n",
    "    #result.strftime(\"%Y-%m-%d\")\n",
    "    return result.date()\n",
    "\n",
    "def extract_map_self_reported(data, data_field, code_map):\n",
    "    pbar = tqdm(total=16)\n",
    "    ### codes\n",
    "    fields = [\"20002\"]; pbar.update(1)\n",
    "    raw = get_data_fields_all(fields, data, data_field); pbar.update(1)\n",
    "    col = \"noncancer_illness_code_selfreported_f20002\"; pbar.update(1)\n",
    "    temp = pd.wide_to_long(raw, stubnames=[col], i=\"eid\", j=\"instance_index\", sep=\"_\", suffix=\"\\w+\").reset_index(); pbar.update(1)\n",
    "    codes = temp.rename(columns={col:\"code\"})\\\n",
    "        .assign(code=lambda x: x.code.astype(str))\\\n",
    "        .replace(\"None\", np.nan) \\\n",
    "        .replace(\"nan\", np.nan) \\\n",
    "        .dropna(subset=[\"code\"], axis=0)\\\n",
    "        .assign(code=lambda x: x.code.astype(int)) \\\n",
    "        .merge(code_map, how=\"left\",on=\"code\") \\\n",
    "        .dropna(subset=[\"meaning\"], axis=0)\\\n",
    "        .sort_values([\"eid\", \"instance_index\"]) \\\n",
    "        .reset_index(drop=True); pbar.update(1)\n",
    "    \n",
    "    ### dates\n",
    "    fields = [\"20008\"]; pbar.update(1)\n",
    "    raw = get_data_fields_all(fields, data, data_field); pbar.update(1)\n",
    "    col=\"interpolated_year_when_noncancer_illness_first_diagnosed_f20008\"; pbar.update(1)\n",
    "    temp = pd.wide_to_long(raw, stubnames=[col], i=\"eid\", j=\"instance_index\", sep=\"_\", suffix=\"\\w+\").reset_index(); pbar.update(1)\n",
    "    dates = temp.rename(columns={col:\"date\"})\\\n",
    "        .dropna(subset=[\"date\"], axis=0)\\\n",
    "        .sort_values([\"eid\", \"instance_index\"]) \\\n",
    "        .reset_index(drop=True); pbar.update(1)\n",
    "\n",
    "    dates = dates[dates.date!=-1]; pbar.update(1)\n",
    "    dates = dates[dates.date!=-3]; pbar.update(1)\n",
    "    dates.date = dates.date.apply(datetime_from_dec_year); pbar.update(1)\n",
    "    \n",
    "    test = codes.merge(dates, how=\"left\", on=[\"eid\", \"instance_index\"]).assign(origin=\"self_reported\").copy(); pbar.update(1)\n",
    "    \n",
    "    test[\"instance_index\"] = test[\"instance_index\"].astype(\"string\"); pbar.update(1)\n",
    "    test[['instance','n']] = test.instance_index.str.split(\"_\",expand=True); pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    return test[[\"eid\", \"origin\", 'instance','n', \"code\", \"meaning\", \"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:46:00.098893Z",
     "start_time": "2020-11-04T12:43:13.432479Z"
    }
   },
   "outputs": [],
   "source": [
    "codes_self_reported = extract_map_self_reported(data, data_field, coding609)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_gp_records = pd.read_feather(f\"{data_path}/1_decoded/codes_gp_diagnoses_210119.feather\").drop(\"level\", axis=1)\n",
    "codes_hospital_records = pd.read_feather(f\"{data_path}/1_decoded/codes_hes_diagnoses_210120.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine diagnoses and events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:46:25.115010Z",
     "start_time": "2020-11-04T12:46:08.961388Z"
    }
   },
   "outputs": [],
   "source": [
    "diagnoses_codes = codes_self_reported.append(codes_hospital_records).append(codes_gp_records).sort_values([\"eid\", \"date\"]).dropna(subset=[\"date\"], axis=0).reset_index(drop=True)\n",
    "diagnoses_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes.reset_index(drop=True).assign(eid = lambda x: x.eid.astype(int),\n",
    "                                              origin = lambda x: x.origin.astype(str),\n",
    "                                              instance = lambda x: x.instance.astype(int),\n",
    "                                              n = lambda x: x.n.astype(int),\n",
    "                                              code = lambda x: x.code.astype(str), \n",
    "                                              meaning = lambda x: x.meaning.astype(str))\\\n",
    "    .to_feather(os.path.join(path, dataset_path, 'temp_diagnoses_codes.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_codes = pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses_codes.feather'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T12:46:26.189927Z",
     "start_time": "2020-11-04T12:46:25.117069Z"
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def had_diagnosis_before_per_ph(df_before, ph, ph_codes, temp):\n",
    "    df_ph = df_before[df_before.meaning.isin(ph_codes)][[\"eid\"]]\\\n",
    "            .drop_duplicates(subset=[\"eid\"])\\\n",
    "            .assign(phenotype=True) \n",
    "    return temp.merge(df_ph, how=\"left\", on=\"eid\").fillna(False).phenotype\n",
    "\n",
    "def had_diagnosis_before(data, diagnoses_codes, phenotypes, time0=time0_col):\n",
    "    diagnoses_codes_time = diagnoses_codes.merge(data[[\"eid\", time0]], how=\"left\", on=\"eid\")\n",
    "    \n",
    "    temp = data[[\"eid\"]].copy()\n",
    "    df_before = diagnoses_codes_time[diagnoses_codes_time.date < diagnoses_codes_time[time0]]\n",
    "                                                                                         \n",
    "    df_phs = Parallel(n_jobs=20, require=\"sharedmem\")(delayed(had_diagnosis_before_per_ph)(df_before, ph, phenotypes[ph], temp) for ph in tqdm(list(phenotypes)))\n",
    "    for ph, df_ph_series in zip(tqdm(list(phenotypes)), df_phs): temp[ph] = df_ph_series\n",
    "    \n",
    "    return temp.sort_values(\"eid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T13:17:52.922023Z",
     "start_time": "2020-11-04T12:46:26.191314Z"
    }
   },
   "outputs": [],
   "source": [
    "diagnoses = had_diagnosis_before(basics, diagnoses_codes, l10, time0=time0_col)\n",
    "\n",
    "diagnoses.to_feather(os.path.join(path, dataset_path, 'temp_diagnoses.feather'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dfs_dict = {\"basics\": pd.read_feather(os.path.join(path, dataset_path, 'temp_basics.feather')), \n",
    "                 \"questionnaire\": pd.read_feather(os.path.join(path, dataset_path, 'temp_questionnaire.feather')), \n",
    "                 \"measurements\": pd.read_feather(os.path.join(path, dataset_path, 'temp_measurements.feather')), \n",
    "                 \"labs\": pd.read_feather(os.path.join(path, dataset_path, 'temp_labs.feather')), \n",
    "                \"metabolomics\": pd.read_feather(os.path.join(path, dataset_path, 'temp_metabolomics.feather')), \n",
    "                 \"family_history\": pd.read_feather(os.path.join(path, dataset_path, 'temp_family_history.feather')), \n",
    "                 \"diagnoses\": pd.read_feather(os.path.join(path, dataset_path, 'temp_diagnoses.feather')),\n",
    "                 \"medications\": pd.read_feather(os.path.join(path, dataset_path, 'temp_medications.feather'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_clean(df):\n",
    "    df.columns = df.columns.str.replace(r'_0_0$', '').str.replace(\"_automated_reading\", '')\n",
    "    fields = df.columns.str.extract(r'_f([0-9]+)$').squeeze().to_list()\n",
    "    df.columns = df.columns.str.replace(r'_f[0-9]+$', '')\n",
    "    return df.columns, fields\n",
    "\n",
    "def clean_df(df):\n",
    "    df.columns, fields = get_cols_clean(df)\n",
    "    return df, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "data_baseline = reduce(lambda x, y: pd.merge(x, y, on = 'eid'), list(data_dfs_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline, fields = clean_df(data_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [col for col in list(data_baseline.columns) if (\"_event\" in col) & (\"_time\" not in col)]:\n",
    "    data_baseline[col] = data_baseline[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = [col for col in list(data_baseline.columns) if not \"_event\" in col]\n",
    "targets = [col for col in list(data_baseline.columns) if \"_event\" in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = {}\n",
    "for topic, df in data_dfs_dict.items(): \n",
    "    data_cols[\"eid\"] = [\"admin\"]\n",
    "    data_cols[topic]=list(get_cols_clean(df)[0])[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_single = {}\n",
    "for topic, columns in data_cols.items():\n",
    "    for col in columns:\n",
    "        data_cols_single[col] = topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\"int32\":\"int\", \"int64\":\"int\", \"float64\":\"float\", \"category\":\"category\", \"object\":\"category\", \"bool\":\"bool\"}\n",
    "desc_dict = {\"id\": [*range(1, len(data_baseline.columns.to_list())+1)] , \n",
    "             \"covariate\": data_baseline.columns.to_list(), \n",
    "             \"dtype\":[dtypes[str(col)] for col in data_baseline.dtypes.to_list()], \n",
    "             \"isTarget\":[True if col in targets else False for col in data_baseline.columns.to_list()],\n",
    "            \"based_on\":[topic for col, topic in data_cols_single.items()],\n",
    "             \"field\": fields,\n",
    "            \"aggr_fn\": [np.nan for col in data_baseline.columns.to_list()]}\n",
    "data_baseline_description = pd.DataFrame.from_dict(desc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {}\n",
    "for group in data_baseline_description.based_on.unique(): feature_dict[group] = data_baseline_description.query(\"based_on==@group\").covariate.to_list()\n",
    "with open(os.path.join(path, dataset_path, 'feature_list.yaml'), 'w') as file: yaml.dump(feature_dict, file, default_flow_style=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_baseline.to_feather(os.path.join(path, dataset_path, 'baseline_covariates.feather'))\n",
    "data_baseline_description.to_feather(os.path.join(path, dataset_path, 'baseline_covariates_description.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!! REMEMBER IMPUTATION !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pl1.x]",
   "language": "python",
   "name": "conda-env-miniconda3-pl1.x-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}